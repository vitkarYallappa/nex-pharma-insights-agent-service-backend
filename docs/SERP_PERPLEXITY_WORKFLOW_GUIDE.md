# SERP-Perplexity Workflow Complete Guide

## ğŸ¯ Overview

This document provides a complete breakdown of the SERP-Perplexity workflow, from API submission to final results. It shows every file, method, and data flow involved in processing your request format.

## ğŸ“‹ Request Format

```json
{
    "project_id": "clinical_trials_test",
    "user_id": "test_user_003",
    "priority": "high",
    "processing_strategy": "table",
    "config": {
        "keywords": [
            "Obesity",
            "Weight loss",
            "Overweight",
            "Obese"
        ],
        "sources": [
            {
                "name": "ClinicalTrials",
                "type": "clinical",
                "url": "https://clinicaltrials.gov/"
            },
            {
                "name": "PubMed",
                "type": "clinical",
                "url": "https://pubmed.ncbi.nlm.nih.gov/"
            }
        ],
        "extraction_mode": "summary",
        "quality_threshold": 0.5,
        "date_range": {
            "start_date": "2025-08-01",
            "end_date": "2025-08-03"
        }
    }
}
```

## ğŸ”„ Complete Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SERP-PERPLEXITY WORKFLOW                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. API REQUEST
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   POST Request  â”‚
   â”‚   /api/v1/      â”‚â”€â”€â”€â”€â”€â”€â”
   â”‚   market-       â”‚      â”‚
   â”‚   intelligence/ â”‚      â”‚
   â”‚   requests      â”‚      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
2. ROUTE HANDLER            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ market_         â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ intelligence_   â”‚
   â”‚ routes.py       â”‚
   â”‚                 â”‚
   â”‚ submit_request()â”‚â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
3. CONTROLLER               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ market_         â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ intelligence_   â”‚
   â”‚ controller.py   â”‚
   â”‚                 â”‚
   â”‚ submit_request()â”‚â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
4. ROOT ORCHESTRATOR        â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ root_           â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ orchestrator_   â”‚
   â”‚ service.py      â”‚
   â”‚                 â”‚
   â”‚ submit_request()â”‚â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
5. TABLE STRATEGY           â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ table_          â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ strategy.py     â”‚
   â”‚                 â”‚
   â”‚ submit_request()â”‚â”€â”€â”€â”€â”€â”€â”
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
6. DATABASE STORAGE         â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ DynamoDB        â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ market_         â”‚
   â”‚ intelligence_   â”‚
   â”‚ requests        â”‚
   â”‚                 â”‚
   â”‚ Status: PENDING â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            
7. BACKGROUND PROCESSOR
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ table_          â”‚
   â”‚ processor.py    â”‚
   â”‚                 â”‚â”€â”€â”€â”€â”€â”€â”
   â”‚ _processing_    â”‚      â”‚
   â”‚ loop()          â”‚      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
8. WORKFLOW DETECTION       â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ _process_       â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ request()       â”‚
   â”‚                 â”‚
   â”‚ Check: sources? â”‚â”€â”€â”€â”€â”€â”€â”
   â”‚ keywords?       â”‚      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
9. STAGE0 ORCHESTRATOR      â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
   â”‚ stage0_         â”‚â—€â”€â”€â”€â”€â”€â”˜
   â”‚ orchestrator/   â”‚
   â”‚ service.py      â”‚
   â”‚                 â”‚
   â”‚ process_        â”‚â”€â”€â”€â”€â”€â”€â”
   â”‚ request()       â”‚      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                            â”‚
10. INGESTION SERVICE       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ ingestion_      â”‚â—€â”€â”€â”€â”€â”˜
    â”‚ service.py      â”‚
    â”‚                 â”‚
    â”‚ process_        â”‚â”€â”€â”€â”€â”€â”
    â”‚ ingestion()     â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
11. SOURCE PROCESSING       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ FOR EACH SOURCE â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ Source 1:       â”‚â”€â”€â”€â”€â”€â”
    â”‚ ClinicalTrials  â”‚     â”‚
    â”‚                 â”‚     â”‚
    â”‚ Source 2:       â”‚     â”‚
    â”‚ PubMed          â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
12. SERP API CALL           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ stage0_serp/    â”‚â—€â”€â”€â”€â”€â”˜
    â”‚ serp_api.py     â”‚
    â”‚                 â”‚
    â”‚ search_with_    â”‚â”€â”€â”€â”€â”€â”
    â”‚ date_range()    â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
13. SERP RESPONSE           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ URLs Found:     â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ â€¢ URL 1         â”‚â”€â”€â”€â”€â”€â”
    â”‚ â€¢ URL 2         â”‚     â”‚
    â”‚ â€¢ URL 3         â”‚     â”‚
    â”‚ â€¢ ...           â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
14. PERPLEXITY PROCESSING   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ FOR EACH URL    â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ stage0_         â”‚â”€â”€â”€â”€â”€â”
    â”‚ perplexity/     â”‚     â”‚
    â”‚ perplexity_     â”‚     â”‚
    â”‚ api.py          â”‚     â”‚
    â”‚                 â”‚     â”‚
    â”‚ extract_single_ â”‚     â”‚
    â”‚ url()           â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
15. CONTENT EXTRACTION      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ Content Results:â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ â€¢ Title         â”‚â”€â”€â”€â”€â”€â”
    â”‚ â€¢ Content       â”‚     â”‚
    â”‚ â€¢ Confidence    â”‚     â”‚
    â”‚ â€¢ Quality Score â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
16. AGGREGATION             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ Combine Results â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ â€¢ All URLs      â”‚â”€â”€â”€â”€â”€â”
    â”‚ â€¢ All Content   â”‚     â”‚
    â”‚ â€¢ Statistics    â”‚     â”‚
    â”‚ â€¢ Metadata      â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                            â”‚
17. FINAL STORAGE           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚ DynamoDB        â”‚â—€â”€â”€â”€â”€â”˜
    â”‚                 â”‚
    â”‚ Status:         â”‚
    â”‚ COMPLETED       â”‚
    â”‚                 â”‚
    â”‚ Results: {...}  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ File-by-File Method Details

### 1. **API Entry Point**

#### `app/routes/market_intelligence_routes.py`
```python
@router.post("/requests")
async def submit_request(
    request_data: SubmitRequestSchema,
    controller: MarketIntelligenceController = Depends(get_controller)
) -> RequestResponseSchema:
    """
    ğŸ¯ Entry point for all market intelligence requests
    
    Flow:
    1. Validates request schema
    2. Calls controller.submit_request()
    3. Returns request_id and status
    """
```

#### `app/schemas/market_intelligence_schema.py`
```python
class SubmitRequestSchema(BaseModel):
    """
    ğŸ” Request validation schema
    
    Validates:
    - project_id, user_id (required)
    - priority (high/medium/low)
    - processing_strategy (table/sqs)
    - config (MarketIntelligenceRequestConfig)
    """
    
    config: Optional[MarketIntelligenceRequestConfig] = Field(...)
    # â†‘ This includes keywords, sources, date_range
```

### 2. **Business Logic Layer**

#### `app/controllers/market_intelligence_controller.py`
```python
class MarketIntelligenceController:
    
    async def submit_request(self, request_data: SubmitRequestSchema) -> RequestResponseSchema:
        """
        ğŸ¯ Main business logic handler
        
        Flow:
        1. Validates business rules
        2. Creates MarketIntelligenceRequest object
        3. Calls orchestrator_service.submit_request()
        4. Returns response with request_id
        """
        
        # Key method calls:
        await self._validate_submission_rules(request_data)
        mi_request = MarketIntelligenceRequest(...)
        success = await self.orchestrator_service.submit_request(mi_request)
```

### 3. **Root Orchestrator Layer**

#### `app/root_orchestrator/root_orchestrator_service.py`
```python
class RootOrchestratorService:
    
    async def submit_request(self, request: MarketIntelligenceRequest) -> bool:
        """
        ğŸ¯ Strategy pattern implementation
        
        Flow:
        1. Gets appropriate strategy (table/sqs)
        2. Calls strategy.submit_request()
        3. Returns success status
        """
        
        strategy = await self._get_strategy_for_request(request)
        success = await strategy.submit_request(request)
```

#### `app/root_orchestrator/models.py`
```python
class MarketIntelligenceRequestConfig(BaseModel):
    """
    ğŸ” Enhanced configuration model
    
    New fields added:
    - keywords: List[str]
    - sources: List[SourceConfig] 
    - date_range: Optional[DateRangeConfig]
    - search_query: Optional[str]
    - quality_threshold: float
    """
    
class SourceConfig(BaseModel):
    """Source configuration"""
    name: str
    type: str  # clinical, academic, government
    url: str

class DateRangeConfig(BaseModel):
    """Date range for searches"""
    start_date: str  # YYYY-MM-DD
    end_date: str    # YYYY-MM-DD
```

### 4. **Strategy Implementation**

#### `app/root_orchestrator/strategies/table_strategy.py`
```python
class TableProcessingStrategy:
    
    async def submit_request(self, request: MarketIntelligenceRequest) -> bool:
        """
        ğŸ¯ Database storage strategy
        
        Flow:
        1. Stores request in DynamoDB
        2. Sets status to PENDING
        3. Background processor will pick it up
        """
        
        await self.database_client.put_item(
            table_name="market_intelligence_requests",
            item=request.dict()
        )
```

### 5. **Background Processing**

#### `app/root_orchestrator/workers/table_processor.py`
```python
class TableProcessor:
    
    async def _processing_loop(self):
        """
        ğŸ”„ Main polling loop
        
        Flow:
        1. Polls database for PENDING requests
        2. Calls _process_request() for each
        3. Sleeps and repeats
        """
        
    async def _process_request(self, request: MarketIntelligenceRequest):
        """
        ğŸ¯ Individual request processing
        
        Flow:
        1. Updates status to PROCESSING
        2. Detects workflow type (SERP-Perplexity vs original)
        3. Calls appropriate workflow
        4. Updates status to COMPLETED/FAILED
        """
        
        # Key detection logic:
        if hasattr(request.config, 'sources') and request.config.sources:
            # Use SERP-Perplexity workflow
            stage0_request = await self._convert_to_stage0_request(request)
            result = await orchestrator_service.process_request(...)
        else:
            # Use original workflow
            result = await self.market_intelligence_service.execute_semaglutide_intelligence(...)
    
    async def _convert_to_stage0_request(self, request: MarketIntelligenceRequest) -> dict:
        """
        ğŸ”„ Request format conversion
        
        Converts:
        - Root orchestrator format â†’ stage0_orchestrator format
        - Handles keywords, sources, date_range mapping
        """
```

### 6. **Stage0 Orchestrator Integration**

#### `app/agent_service_module/agents/stage0_orchestrator/service.py`
```python
class OrchestratorService:
    
    async def process_request(self, query: str, num_results: int = 10, **kwargs) -> IngestionResponse:
        """
        ğŸ¯ Stage0 orchestrator entry point
        
        Flow:
        1. Creates IngestionRequest
        2. Calls ingestion_service.process_ingestion()
        3. Returns IngestionResponse
        """
        
        request = IngestionRequest(
            query=query,
            num_results=num_results,
            keywords=kwargs.get('keywords'),
            sources=kwargs.get('sources'),
            date_range=kwargs.get('date_range'),
            **kwargs
        )
```

#### `app/agent_service_module/agents/stage0_orchestrator/models.py`
```python
class IngestionRequest(BaseModel):
    """
    ğŸ” Enhanced ingestion request
    
    New fields added:
    - keywords: Optional[List[str]]
    - sources: Optional[List[SourceConfig]]
    - date_range: Optional[DateRangeConfig]
    - quality_threshold: float
    """
    
    def is_serp_perplexity_workflow(self) -> bool:
        """Detects if this should use SERP-Perplexity workflow"""
        return bool(self.keywords and self.sources)
```

### 7. **Ingestion Processing**

#### `app/agent_service_module/agents/stage0_orchestrator/ingestion_service.py`
```python
class IngestionService:
    
    async def process_ingestion(self, request: IngestionRequest) -> IngestionResponse:
        """
        ğŸ¯ Main ingestion orchestrator
        
        Flow:
        1. Detects workflow type
        2. For SERP-Perplexity: processes sources one by one
        3. For each source: SERP â†’ URLs â†’ Perplexity â†’ Content
        4. Aggregates all results
        """
        
        if request.is_serp_perplexity_workflow():
            return await self._process_serp_perplexity_workflow(request)
        else:
            return await self._process_standard_workflow(request)
    
    async def _process_serp_perplexity_workflow(self, request: IngestionRequest):
        """
        ğŸ”„ SERP-Perplexity workflow implementation
        
        Flow:
        1. For each source in request.sources:
           a. Call SERP API to find URLs
           b. For each URL found:
              - Call Perplexity API to extract content
              - Apply quality filtering
           c. Aggregate source results
        2. Combine all source results
        3. Return final response
        """
```

### 8. **SERP API Integration**

#### `app/agent_service_module/agents/stage0_serp/serp_api.py`
```python
class SerpAPI:
    
    async def search_with_date_range(
        self, 
        keywords: List[str], 
        source: Dict[str, Any] = None,
        start_date: str = None, 
        end_date: str = None
    ) -> SerpResponse:
        """
        ğŸ” SERP API call with date filtering
        
        Flow:
        1. Converts source to dict format
        2. Builds query with keywords (OR logic)
        3. Adds site restriction (site:domain.com)
        4. Adds date range filter
        5. Calls Google via SerpAPI
        6. Returns structured results
        """
        
        # Key transformations:
        query = '("Obesity" OR "Weight loss" OR "Overweight" OR "Obese") (site:clinicaltrials.gov)'
        params = {
            'tbs': 'cdr:1,cd_min:8/01/2025,cd_max:8/03/2025'
        }
```

#### `app/agent_service_module/agents/stage0_serp/serp_query_builder.py`
```python
def build_date_range_query(
    keywords: List[str], 
    sources: List[Dict[str, Any]] = None,
    start_date: str = None, 
    end_date: str = None
) -> Dict[str, Any]:
    """
    ğŸ”§ Query builder for Google search
    
    Builds:
    - Keywords with OR logic: ("Obesity" OR "Weight loss")
    - Site restrictions: (site:clinicaltrials.gov)
    - Date filters: tbs=cdr:1,cd_min:8/01/2025,cd_max:8/03/2025
    """
```

### 9. **Perplexity API Integration**

#### `app/agent_service_module/agents/stage0_perplexity/perplexity_api.py`
```python
class PerplexityAPI:
    
    async def extract_single_url(self, url: str) -> PerplexityResponse:
        """
        ğŸ” Perplexity API call for content extraction
        
        Flow:
        1. Creates focused query for the URL
        2. Calls Perplexity chat completions API
        3. Extracts content, title, confidence
        4. Applies quality scoring
        5. Returns structured response
        """
        
        # Key API call:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {"role": "system", "content": "Extract key information..."},
                {"role": "user", "content": f"Analyze this URL: {url}"}
            ],
            "return_citations": True,
            "search_domain_filter": [url]
        }
```

## ğŸ“Š Data Flow Diagram

```
REQUEST DATA TRANSFORMATION FLOW:

1. API Request Format:
   {
     "config": {
       "keywords": ["Obesity", "Weight loss"],
       "sources": [{"name": "ClinicalTrials", "url": "..."}],
       "date_range": {"start_date": "2025-08-01", "end_date": "2025-08-03"}
     }
   }
   â†“

2. MarketIntelligenceRequest Object:
   MarketIntelligenceRequest(
     config=MarketIntelligenceRequestConfig(
       keywords=["Obesity", "Weight loss"],
       sources=[SourceConfig(name="ClinicalTrials", ...)],
       date_range=DateRangeConfig(start_date="2025-08-01", ...)
     )
   )
   â†“

3. Database Storage (DynamoDB):
   {
     "request_id": "req_123456",
     "status": "PENDING",
     "config": { ... },
     "created_at": "2025-01-01T10:00:00Z"
   }
   â†“

4. Background Processor Detection:
   if request.config.sources and request.config.keywords:
       use_serp_perplexity_workflow = True
   â†“

5. Stage0 Request Conversion:
   {
     "query": '("Obesity" OR "Weight loss")',
     "keywords": ["Obesity", "Weight loss"],
     "sources": [{"name": "ClinicalTrials", "type": "clinical", "url": "..."}],
     "date_range": {"start_date": "2025-08-01", "end_date": "2025-08-03"}
   }
   â†“

6. SERP API Query:
   {
     "q": '("Obesity" OR "Weight loss") (site:clinicaltrials.gov)',
     "tbs": "cdr:1,cd_min:8/01/2025,cd_max:8/03/2025",
     "engine": "google"
   }
   â†“

7. SERP Response:
   {
     "organic_results": [
       {"title": "Study 1", "link": "https://clinicaltrials.gov/study1"},
       {"title": "Study 2", "link": "https://clinicaltrials.gov/study2"}
     ]
   }
   â†“

8. Perplexity API Calls (for each URL):
   {
     "model": "sonar-pro",
     "messages": [{"role": "user", "content": "Analyze: https://clinicaltrials.gov/study1"}],
     "search_domain_filter": ["https://clinicaltrials.gov/study1"]
   }
   â†“

9. Perplexity Responses:
   {
     "choices": [{
       "message": {
         "content": "This clinical trial studies obesity treatment..."
       }
     }]
   }
   â†“

10. Final Aggregated Results:
    {
      "request_id": "req_123456",
      "status": "completed",
      "sources_processed": [
        {
          "source_name": "ClinicalTrials",
          "urls_found": 5,
          "content_extracted": 3,
          "content": [
            {"url": "...", "title": "...", "content": "...", "confidence": 0.85},
            {"url": "...", "title": "...", "content": "...", "confidence": 0.92}
          ]
        }
      ],
      "total_urls_found": 5,
      "total_content_extracted": 3
    }
```

## â±ï¸ Processing Timeline

```
Timeline for Processing Your Request:

T+0s    â”‚ API Request Received
        â”‚ â”œâ”€ Schema Validation
        â”‚ â”œâ”€ Controller Processing  
        â”‚ â”œâ”€ Root Orchestrator
        â”‚ â””â”€ Database Storage (Status: PENDING)
        â”‚
T+1s    â”‚ Background Processor Picks Up Request
        â”‚ â”œâ”€ Status Update: PROCESSING
        â”‚ â”œâ”€ Workflow Detection: SERP-Perplexity
        â”‚ â””â”€ Stage0 Orchestrator Called
        â”‚
T+2s    â”‚ Source 1 Processing: ClinicalTrials
        â”‚ â”œâ”€ SERP API Call (keywords + site + date range)
        â”‚ â”œâ”€ Response: 5 URLs found
        â”‚ â””â”€ Start Perplexity processing
        â”‚
T+5s    â”‚ Perplexity Processing URLs 1-5
        â”‚ â”œâ”€ URL 1: Content extracted (confidence: 0.85)
        â”‚ â”œâ”€ URL 2: Content extracted (confidence: 0.92)
        â”‚ â”œâ”€ URL 3: Content too short (skipped)
        â”‚ â”œâ”€ URL 4: Content extracted (confidence: 0.78)
        â”‚ â””â”€ URL 5: API error (skipped)
        â”‚
T+15s   â”‚ Source 2 Processing: PubMed
        â”‚ â”œâ”€ SERP API Call (keywords + site + date range)
        â”‚ â”œâ”€ Response: 3 URLs found
        â”‚ â””â”€ Start Perplexity processing
        â”‚
T+20s   â”‚ Perplexity Processing URLs 1-3
        â”‚ â”œâ”€ URL 1: Content extracted (confidence: 0.88)
        â”‚ â”œâ”€ URL 2: Content extracted (confidence: 0.91)
        â”‚ â””â”€ URL 3: Content extracted (confidence: 0.76)
        â”‚
T+25s   â”‚ Results Aggregation
        â”‚ â”œâ”€ Combine all source results
        â”‚ â”œâ”€ Calculate statistics
        â”‚ â”œâ”€ Apply quality filtering
        â”‚ â””â”€ Generate final report
        â”‚
T+26s   â”‚ Final Storage
        â”‚ â”œâ”€ Status Update: COMPLETED
        â”‚ â”œâ”€ Results stored in database
        â”‚ â””â”€ Processing complete
```

## ğŸ” Key Configuration Points

### 1. **SERP API Configuration**
```python
# In stage0_serp/serp_api.py
SERP_CONFIG = {
    "api_key": "sadsad",
    "timeout": 60,
    "max_results": 20,
    "headers": {
        "User-Agent": "Mozilla/5.0...",
        "Accept": "application/json"
    }
}
```

### 2. **Perplexity API Configuration**
```python
# In stage0_perplexity/perplexity_api.py
PERPLEXITY_CONFIG = {
    "api_key": "dumypasdsadplx-asdsadsadsa",
    "model": "sonar-pro",
    "timeout": 60,
    "return_citations": True,
    "return_images": False
}
```

### 3. **Quality Thresholds**
```python
# Content quality filtering
QUALITY_THRESHOLDS = {
    "min_content_length": 100,      # characters
    "min_confidence": 0.5,          # 0.0 - 1.0
    "quality_threshold": 0.5        # from your request
}
```

## ğŸš€ Testing Your Request

### 1. **Start the Server**
```bash
python3 start_server.py
```

### 2. **Submit Your Request**
```bash
curl -X POST http://localhost:8005/api/v1/market-intelligence/requests \
  -H "Content-Type: application/json" \
  -d '{
    "project_id": "clinical_trials_test",
    "user_id": "test_user_003",
    "priority": "high",
    "processing_strategy": "table",
    "config": {
      "keywords": ["Obesity", "Weight loss", "Overweight", "Obese"],
      "sources": [
        {
          "name": "ClinicalTrials",
          "type": "clinical",
          "url": "https://clinicaltrials.gov/"
        },
        {
          "name": "PubMed",
          "type": "clinical",
          "url": "https://pubmed.ncbi.nlm.nih.gov/"
        }
      ],
      "extraction_mode": "summary",
      "quality_threshold": 0.5,
      "date_range": {
        "start_date": "2025-08-01",
        "end_date": "2025-08-03"
      }
    }
  }'
```

### 3. **Check Status**
```bash
curl http://localhost:8005/api/v1/market-intelligence/requests/{request_id}/status
```

### 4. **Get Results**
```bash
curl http://localhost:8005/api/v1/market-intelligence/requests/{request_id}/results
```

## ğŸ“‹ Expected Response Format

```json
{
  "request_id": "req_20250101_123456",
  "status": "completed",
  "started_at": "2025-01-01T10:00:00Z",
  "completed_at": "2025-01-01T10:00:26Z",
  "sources_processed": [
    {
      "source_name": "ClinicalTrials",
      "source_url": "https://clinicaltrials.gov/",
      "source_type": "clinical",
      "urls_found": 5,
      "content_extracted": 3,
      "urls": [
        "https://clinicaltrials.gov/study/NCT123456",
        "https://clinicaltrials.gov/study/NCT789012"
      ],
      "content": [
        {
          "url": "https://clinicaltrials.gov/study/NCT123456",
          "title": "Obesity Treatment Study",
          "content": "This randomized controlled trial...",
          "confidence": 0.85,
          "quality_score": 0.78,
          "keywords_found": ["Obesity", "Weight loss"]
        }
      ]
    },
    {
      "source_name": "PubMed",
      "source_url": "https://pubmed.ncbi.nlm.nih.gov/",
      "source_type": "clinical",
      "urls_found": 3,
      "content_extracted": 3,
      "content": [...]
    }
  ],
  "total_urls_found": 8,
  "total_content_extracted": 6,
  "processing_time_seconds": 26,
  "errors": [],
  "warnings": ["Content too short from 2 URLs"]
}
```

## ğŸ¯ Summary

This workflow successfully integrates:

âœ… **Your Request Format** - Keywords, sources, date ranges  
âœ… **SERP API** - Finds URLs from each source  
âœ… **Perplexity API** - Extracts content from each URL  
âœ… **Source-by-Source Processing** - Handles sources one by one  
âœ… **Quality Filtering** - Applies your quality threshold  
âœ… **Existing Infrastructure** - Uses current API endpoint  

The system processes your sources sequentially (ClinicalTrials first, then PubMed), calls SERP to find URLs, then calls Perplexity for each URL to extract content, exactly as requested! ğŸš€ 